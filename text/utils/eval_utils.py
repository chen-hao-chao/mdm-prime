import os
import math
import torch
import mauve
import typing
import transformers
import torch.nn.functional as F
from fast_bleu import SelfBLEU
from collections import Counter
from sentence_transformers import SentenceTransformer

def compute_mauve(
    text_samples: typing.List[str],
    text_reference: typing.List[str], 
    logger=None,
    model_type="all-mpnet-base-v2",
    device_id=-1,
    mauve_scaling_factor=5) -> None:
    """
    Compute the MAUVE score between generated text samples and reference text.
    
    Args:
        text_samples (List[str]): List of generated text samples.
        text_reference (List[str]): List of reference texts.
    """
    # Load optimized sentence embedding model
    emb_model = SentenceTransformer(model_type)  

    emb_samples = emb_model.encode(text_samples, convert_to_numpy=True)
    emb_reference = emb_model.encode(text_reference, convert_to_numpy=True)
    if logger:
        logger.info("sample embedding size: {}".format(emb_samples.shape))
        logger.info("reference embedding size: {}".format(emb_reference.shape))    
    mauve_score = mauve.compute_mauve(p_features=emb_samples, q_features=emb_reference, 
                                        device_id=device_id, mauve_scaling_factor=mauve_scaling_factor)
    return mauve_score

def compute_self_bleu(model, sentences: typing.List[str], mode='4-gram', logger=None):
    """
    Compute Self-BLEU for a given list of generated sentences.

    Args:
        sentences (list of str): The generated sentences.
        n_grams (int): Maximum n-gram to consider for BLEU score (default=4).
    
    Returns:
        float: The average Self-BLEU score.
    """
    (references, _, _) = model.eval_retokenize(sentences, max_length=model.config.model.length)
    references = references.tolist()
    weights = {'4-gram': (1/4., 1/4., 1/4., 1/4.)}
    self_bleu = SelfBLEU(references, weights).get_score()[mode]
    return sum(self_bleu) / len(self_bleu)

# Source: https://github.com/facebookresearch/flow_matching/blob/main/examples/text/logic/evaluate.py
def _sample_entropy(sample: typing.List) -> float:
    histogram = Counter(sample)
    total = sum(histogram.values())
    entropy = 0

    for count in histogram.values():
        p = count / total
        entropy -= p * math.log2(p)

    return entropy

def compute_entropy(model, sentences: typing.List[str], logger=None) -> float:
    entropies = []
    (samples, _, _) = model.eval_retokenize(sentences, max_length=model.config.model.length)
    for sample in samples:
        entropy = _sample_entropy(sample.tolist())
        entropies.append(entropy)

    return sum(entropies) / len(entropies)

def compute_ngram_repetition_percentage(sentences, n=1):
    """
    Compute the percentage of repeated n-grams in the text.

    Args:
        sentences (List[str]): List of generated sentences.
        n (int): N-gram size (default=2).

    Returns:
        float: Percentage of repeated n-grams (0%â€“100%).
    """
    total_repeat_ratio = []
    
    for sentence in sentences:
        words = sentence.split()
        ngrams = [tuple(words[i:i+n]) for i in range(len(words) - n + 1)]
        ngram_counts = Counter(ngrams)
        repeat_ngrams = sum(count - 1 for count in ngram_counts.values() if count > 1)
        total_ngrams = len(ngrams)
        repeat_ratio = (repeat_ngrams / total_ngrams) * 100 if total_ngrams > 0 else 0
        total_repeat_ratio.append(repeat_ratio)

    return sum(total_repeat_ratio) / len(total_repeat_ratio)

def compute_generative_perplexity(
    model,
    text_samples: typing.List[str],
    retokenize: bool = True,
    max_length: typing.Optional[int] = None,
    logger = None) -> None:
    """Compute the generative perplexity of the model.

    Args:
        text_samples: List of sentences generated by the model.
    
    Returns:
        Perplexity of the generated text under a different
        pre-trained AR model (e.g., GPT2).
    """
    os.environ['TOKENIZERS_PARALLELISM'] = 'false'

    if model.gen_ppl_eval_model_name_or_path == "GSAI-ML/LLaDA-8B-Base":
        eval_model = transformers.AutoModel.from_pretrained(model.gen_ppl_eval_model_name_or_path, trust_remote_code=True, torch_dtype=torch.bfloat16).eval()
        empty_prompt = torch.tensor([]).to(model.device)
    else:
        eval_model = transformers.AutoModelForCausalLM.from_pretrained(model.gen_ppl_eval_model_name_or_path).eval()
    if max_length is None:
        max_length = model.config.model.length
    if 'llama2' not in model.gen_ppl_eval_model_name_or_path:
        eval_model = eval_model.to(model.device)
    # Re-tokenize using eval model's tokenizer
    if retokenize:
        (samples, attn_mask, eval_context_size) = model.eval_retokenize(text_samples, max_length=max_length)
    else:
        samples = text_samples
        attn_mask = torch.ones(samples.shape).to(model.device)
        eval_context_size = samples.shape[-1]
    
    batch_size = min(model.config.eval.perplexity_batch_size, samples.shape[0]) if model.gen_ppl_eval_model_name_or_path != "GSAI-ML/LLaDA-8B-Base" else 1
    num_batches = samples.shape[0] // batch_size
    for i in range(num_batches):
        if logger:
            logger.info("evaluating {} / {} samples ...".format(i, num_batches))
        _samples = torch.split(samples[i * batch_size: (i + 1) * batch_size], eval_context_size, dim=-1)
        _attn_mask = torch.split(attn_mask[i * batch_size: (i + 1) * batch_size], eval_context_size, dim=-1)
        for (sample_chunk, attn_mask_chunk) in zip(_samples, _attn_mask):
            if model.gen_ppl_eval_model_name_or_path == "GSAI-ML/LLaDA-8B-Base":
                nlls = get_log_likelihood(eval_model, empty_prompt, sample_chunk[0], mc_num=128)
                model.gen_ppl_metric.update(nlls)
            else:
                logits = eval_model(sample_chunk, attention_mask=attn_mask_chunk)[0]
                logits = logits.transpose(-1, -2)
                nlls = F.cross_entropy(logits[..., :-1], sample_chunk[..., 1:], reduction='none')
                first_eos = (sample_chunk == model.eval_model_tokenizer.eos_token_id).cumsum(-1) == 1
                token_mask = (sample_chunk != model.eval_model_tokenizer.eos_token_id)
                mask = first_eos[..., 1:] + token_mask[..., 1:]
                model.gen_ppl_metric.update(nlls, mask)
        
    gen_ppl = model.gen_ppl_metric.compute()
    return gen_ppl.item()

# ---

# Source: https://github.com/ML-GSAI/LLaDA/blob/main/get_log_likelihood.py
import torch
import torch.nn.functional as F

def forward_process(batch, prompt_index, mask_id):
    b, l = batch.shape

    target_len = (l - prompt_index.sum()).item()
    k = torch.randint(1, target_len + 1, (), device=batch.device)

    x = torch.round(torch.linspace(float(k), k + (b - 1) * (target_len / b), steps=b, device=batch.device)).long()
    x = ((x - 1) % target_len) + 1
    assert x.min() >= 1 and x.max() <= target_len

    indices = torch.arange(target_len, device=batch.device).repeat(b, 1)
    is_mask = indices < x.unsqueeze(1)
    for i in range(b):
        is_mask[i] = is_mask[i][torch.randperm(target_len)]

    is_mask = torch.cat((torch.zeros(b, prompt_index.sum(), dtype=torch.bool, device=batch.device), is_mask), dim=1)
    noisy_batch = torch.where(is_mask, mask_id, batch)

    # Return the masked batch and the mask ratio
    return noisy_batch, (x / target_len).unsqueeze(1).repeat(1, l)


def get_logits(model, batch, prompt_index, cfg_scale, mask_id):
    if cfg_scale > 0.:
        assert len(prompt_index) == batch.shape[1]
        prompt_index = prompt_index.unsqueeze(0).repeat(batch.shape[0], 1)
        un_batch = batch.clone()
        un_batch[prompt_index] = mask_id
        batch = torch.cat([batch, un_batch])

    input = batch
    logits = model(input).logits

    if cfg_scale > 0.:
        logits, un_logits = torch.chunk(logits, 2, dim=0)
        logits = un_logits + (cfg_scale + 1) * (logits - un_logits)
    return logits


@ torch.no_grad()
def get_log_likelihood(model, prompt, answer, mc_num=128, batch_size=16, cfg_scale=0., mask_id=126336, length=1024):
    '''
    Args:
        model: Mask predictor.
        prompt: A tensor of shape (l1).
        answer: A tensor of shape (l2).
        mc_num: Monte Carlo estimation times.
                As detailed in Appendix B.5. Since MMLU, CMMLU, and C-EVAL only require the likelihood of a single token, a
                single Monte Carlo estimate is sufficient for these benchmarks. For all other benchmarks, we find that 128
                Monte Carlo samples are adequate to produce stable results.
        batch_size: Mini batch size.
        cfg_scale: Unsupervised classifier-free guidance scale.
        mask_id: The toke id of [MASK] is 126336.
    '''
    seq = torch.concatenate([prompt, answer])[None, :]
    seq = seq.repeat((batch_size, 1)).to(model.device, dtype=torch.long)
    prompt_index = torch.arange(seq.shape[1], device=model.device) < len(prompt)

    loss_ = torch.zeros((1, length)).to(model.device)
    for _ in range(mc_num // batch_size):
        perturbed_seq, p_mask = forward_process(seq, prompt_index, mask_id)
        mask_index = (perturbed_seq == mask_id)

        logits = get_logits(model, perturbed_seq, prompt_index, cfg_scale, mask_id)
        logits = logits.transpose(1, 2)

        loss = F.cross_entropy(logits, seq, reduction='none') / p_mask
        loss[mask_index] = 0
        loss_ += loss.sum(0) / mc_num

    return loss_
